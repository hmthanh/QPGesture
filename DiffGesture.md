# Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation

## Abstract

* Animating virtual avatars with co-speech gestures has applications in human-machine interaction.
* Existing methods using generative adversarial networks (GANs) suffer from mode collapse and unstable training, limiting accurate audio-gesture joint distribution learning.
* Proposed Diffusion Co-Speech Gesture (DiffGesture) framework effectively captures cross-modal audio-to-gesture associations and preserves temporal coherence for high-fidelity audio-driven co-speech gesture generation.
* Diffusion-conditional generation process established on skeleton sequences and audio.
* Diffusion Audio-Gesture Transformer designed to attend to information from multiple modalities and model long-term temporal dependency.
* Diffusion Gesture Stabilizer proposed to eliminate temporal inconsistency with annealed noise sampling strategy.
* Incorporates implicit classifier-free guidance for diversity and gesture quality trade-off.
* Extensive experiments demonstrate that DiffGesture achieves state-of-the-art performance with coherent gestures, better mode coverage, and stronger audio correlations.

## 1. Introduction

## 2. Related Work

## 3. Our Approach

## 4. Experiments

## 5. Discussion and Conclusion

